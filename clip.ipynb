{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ['abstract', 'cow']\n",
    "\n",
    "text_inputs = tokenizer(samples, padding='max_length', return_tensors=\"pt\", max_length=10)\n",
    "\n",
    "model.get_text_features(**text_inputs.to(device)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPTextConfig\n",
    "\n",
    "class CLIPForRegression(CLIPTextModelWithProjection):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.regressor = torch.nn.Linear(config.hidden_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        outputs = super().forward(**kwargs)\n",
    "        outputs = self.dropout(outputs.text_embeds)\n",
    "        outputs = self.regressor(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/martingrzzler___json/martingrzzler--conreteness_ratings-8e85e116392013eb/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c48286c6fd49138b57afa444a1fd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('martingrzzler/conreteness_ratings')\n",
    "ds = ds.rename_column('Conc.M', 'avg_concreteness')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets import Dataset \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "processing_ds = ds['train'].to_pandas()\n",
    "\n",
    "processing_ds['avg_concreteness'] = scaler.fit_transform(processing_ds['avg_concreteness'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "ds['train'] = Dataset.from_pandas(processing_ds)\n",
    "\n",
    "\n",
    "ds = ds.remove_columns(['Conc.SD', 'Unknown', 'Total', 'SUBTLEX', 'Percent_known', 'Bigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f7a53eddbd4edfb26325224fa22c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/39953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['Word'], padding='max_length', return_tensors=\"pt\", max_length=10)\n",
    "\n",
    "ds = ds.filter(lambda x: x['Word'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db30d107bda4d89ace7eb4321afbf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize, batched=True)\n",
    "tokenized_ds = tokenized_ds.rename_column('avg_concreteness', 'labels')\n",
    "tokenized_ds = tokenized_ds['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "tokenized_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(tokenized_ds['train'], batch_size=8)\n",
    "test_loader = DataLoader(tokenized_ds['test'], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPForRegression: ['vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'logit_scale', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias']\n",
      "- This IS expected if you are initializing CLIPForRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPForRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CLIPForRegression were not initialized from the model checkpoint at openai/clip-vit-base-patch32 and are newly initialized: ['regressor.bias', 'regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPForRegression(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (regressor): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLIPForRegression.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 4\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(name='linear',optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad499f8474c4a43a7d7b7707fd20886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = loss_fn(outputs, batch['labels'].unsqueeze(-1)) \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f16d99e5d00>]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: 0.9007\n",
      "P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "    predictions.extend(outputs.squeeze().tolist())\n",
    "    targets.extend(batch[\"labels\"].squeeze().tolist())\n",
    "\n",
    "\n",
    "\n",
    "corr_corf, p_value = pearsonr(predictions, targets)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient: {corr_corf:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenizer(word, padding='max_length', return_tensors=\"pt\", max_length=10).to(device))\n",
    "    return outputs.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9851014614105225, 0.18514317274093628, 0.9905253648757935)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('dog'), predict('abstract'), predict('cow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/martingrzzler/clip-word-concreteness/commit/6f7263d205136f76fb590eb5fc97ac74fa1271cb', commit_message='Upload CLIPForRegression', commit_description='', oid='6f7263d205136f76fb590eb5fc97ac74fa1271cb', pr_url='https://huggingface.co/martingrzzler/clip-word-concreteness/discussions/1', pr_revision='refs/pr/1', pr_num=1)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('martingrzzler/clip-word-concreteness', create_pr=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
