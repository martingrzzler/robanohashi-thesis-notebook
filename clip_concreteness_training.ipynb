{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca58fa456ec4007ba3d2baeef172791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e886f3f841c4ce499c127058a256f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d909c75e8b4dbfafa856c5d6d44e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373ea5c1d48c4a23b1fa301759a6214e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b457f46f4048e69600c6ab52760d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "import torch\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPTextConfig\n",
    "\n",
    "class CLIPForRegression(CLIPTextModelWithProjection):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__(config)\n",
    "        self.regressor = torch.nn.Linear(config.hidden_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        outputs = super().forward(**kwargs)\n",
    "        outputs = self.dropout(outputs.text_embeds)\n",
    "        outputs = self.regressor(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16d743adf4d4c888b5d273f68f0aec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/145 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/martingrzzler--conreteness_ratings to /root/.cache/huggingface/datasets/martingrzzler___json/martingrzzler--conreteness_ratings-8e85e116392013eb/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ba56173f124b31b451736849005749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1c118878c844d79a455b0f01c514ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcb81c82c5d48089cdc552eeaa20aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672756d9399d4af384c1c5a845d46320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/martingrzzler___json/martingrzzler--conreteness_ratings-8e85e116392013eb/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c356fac0c9904f9da272d4a5ba870829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('martingrzzler/conreteness_ratings')\n",
    "ds = ds.rename_column('Conc.M', 'avg_concreteness')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets import Dataset \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "processing_ds = ds['train'].to_pandas()\n",
    "\n",
    "processing_ds['avg_concreteness'] = scaler.fit_transform(processing_ds['avg_concreteness'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "ds['train'] = Dataset.from_pandas(processing_ds)\n",
    "\n",
    "\n",
    "ds = ds.remove_columns(['Conc.SD', 'Unknown', 'Total', 'SUBTLEX', 'Percent_known', 'Bigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035cf405ad8c4f3da5b750aafe5c4048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/39954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['Word'], padding='max_length', return_tensors=\"pt\", max_length=10)\n",
    "\n",
    "ds = ds.filter(lambda x: x['Word'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d47a36439f47a6b4b30c3abe63bdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize, batched=True)\n",
    "tokenized_ds = tokenized_ds.rename_column('avg_concreteness', 'labels')\n",
    "tokenized_ds = tokenized_ds['train'].train_test_split(test_size=0.1, shuffle=True)\n",
    "tokenized_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(tokenized_ds['train'], batch_size=8)\n",
    "test_loader = DataLoader(tokenized_ds['test'], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7714fa0b8c04acb891c9663a49d0980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPForRegression: ['vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'logit_scale', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight']\n",
      "- This IS expected if you are initializing CLIPForRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPForRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CLIPForRegression were not initialized from the model checkpoint at openai/clip-vit-base-patch32 and are newly initialized: ['regressor.bias', 'regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPForRegression.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 4\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(name='linear',optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e01121b914462fa74613a893ceb55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = loss_fn(outputs, batch['labels'].unsqueeze(-1)) \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4f5cd6a90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: 0.9049\n",
      "P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "    predictions.extend(outputs.squeeze().tolist())\n",
    "    targets.extend(batch[\"labels\"].squeeze().tolist())\n",
    "\n",
    "\n",
    "\n",
    "corr_corf, p_value = pearsonr(predictions, targets)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient: {corr_corf:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenizer(word, padding='max_length', return_tensors=\"pt\", max_length=10).to(device))\n",
    "    return outputs.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9851014614105225, 0.18514317274093628, 0.9905253648757935)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('dog'), predict('abstract'), predict('cow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/martingrzzler/clip-word-concreteness/commit/6f7263d205136f76fb590eb5fc97ac74fa1271cb', commit_message='Upload CLIPForRegression', commit_description='', oid='6f7263d205136f76fb590eb5fc97ac74fa1271cb', pr_url='https://huggingface.co/martingrzzler/clip-word-concreteness/discussions/1', pr_revision='refs/pr/1', pr_num=1)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('martingrzzler/clip-word-concreteness', create_pr=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
